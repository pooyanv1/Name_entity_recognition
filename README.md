# Name_entity_recognition
Please run the cells in a Google colab notebook. For storing the results in a database, the commands need to get executed locally which have also been specified by the comments in top of the cells. As specified in the assignment, TensorFlow is utilized. 
The model is influenced by the 2017 paper "attention is all you need". The transformer used for this model utilizes the "multihead attention" mechanism. First, the input words are embedded and added to the positional encodings. the addition result is fed into the multi-head attention layer. The output is normalized and added to the initial addition result. The results are fed into a feedforward layer followed by a normalization layer. Moreover, the model uses 4 transformers which help improve the efficiency of the model. This idea was based upon the structure of BERT which uses either 12 or 24 encoders. Thus, the previous calculations are performed in sequence four times.
The dataset used for this work contains is the conll2003 dataset which has training, validation, and testing data. However, the latter is not used as the news article data has been utilized for testing purposes.
The tag_dict created is based upon the Inside-outside-Beginning (IoB) format. However, the MISC tag in the dataset is not required for this task and only location, name, and organization name were considered.
For the web scraping part, the text data is extracted from an NBC news article regarding COVID-19. Moreover, the data is transformed in a way that it could get fed into the model and achieve predictions.
For creating the database, the pickle files need to get stored in the same directory as where the code of the last cell is stored. The pickle files were created to store the input and output of the test data and used to create the database locally.
